---
output:
    html_document:
        toc: true
        toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview of HPC at USC

<figure>

<img src="figs/CARC-cyberinfrastructure.png" alt="Cyberinfrastructure overview" style="width:80%;"/>

<figcaption>

Cyberinfrastructure overview (source: <https://carc.usc.edu/user-information/system-information>)

</figcaption>

</figure>

# Hello world

For a quick-n-dirty intro to Slurm, we will start with a simple "Hello world"
using Slurm + R. For this, we need to go through the following steps:

1.  Copy a Slurm script to HPC,

2.  Login to HPC, and

3.  Submit the job using `sbatch`.


## Step 1: Copy the Slurm script to HPC

We need to copy the following Slurm script to HPC:

```{r helloworld, echo=FALSE, comment = '', warning=FALSE}
cat(readLines("00-hello-world.slurmr"), sep = "\n")
```

Which has four lines:

1.  `#!/bin/sh` the **shebang** ([**shewhat?**](https://stackoverflow.com/questions/7366775/what-does-the-line-bin-sh-mean-in-a-unix-shell-script))

2.  `#SBATCH --output=00-hello-world.out` an option to be passed to `sbatch`, in
    this case, the name of the output file to which
    [**stdout and stderr**](https://en.wikipedia.org/wiki/Standard_streams) will go.
    
3.  `module load usc r` uses [**Lmod**](https://lmod.readthedocs.io/en/latest/) to load the `usc` (required) and `R`
    modules.
    
4.  `Rscript ...` a call to R to evaluate the expression `paste(...)`. This will
    get the environment variable `SLURMD_NODENAME` (which `sbatch` creates) and
    print it on a message.


To do so, we will use **Secure copy protocol (scp)**, which allows us to copy
data to and fro computers. In this case, we should do something like the following

```bash
scp 00-hello-world.slurmr vegayon@hpc-transfer1.usc.edu:/home1/vegayon/
```

In words, "Using the username `vegayon`, connect to `hpc-transfer1.usc.edu`, take
the file `00-hello-world.slurmr` and copy it to `/home1/vegayon/`. With the file
now available in the cluster, we can submit this job using Slurm.


## Step 2: Loging in

1.  Log-in using ssh. In the case of Windows users, download the
    [**Putty**](https://www.chiark.greenend.org.uk/~sgtatham/putty/) client. You have
    two options, the
    [**discovery**](https://carc.usc.edu/user-information/user-guides/high-performance-computing/getting-started-discovery)
    or
    [**endeavour**](https://carc.usc.edu/user-information/user-guides/high-performance-computing/getting-started-endeavour)
    clusters.

2.  To login, you will need to use your USC Net ID. If your USC email is
    `flasname@usc.edu`, your USC Net ID is `flastname`. Then:
    
    ``` {.bash}
    ssh flastname@discovery.usc.edu
    ```
    
    if you want to use the discovery cluster (available to all USC members), or
    
    ``` {.bash}
    ssh flastname@endeavour.usc.edu
    ```
    
    if you want to use the endeavour cluster (using private condos).


## Step 3: Submitting the job

1.  The HPC has several pre-installed pieces of software. R is one of those.

2.  To access the pre-installed software, we use the
    [**Lmod module system**](https://lmod.readthedocs.io/en/latest/) (more information
    [**here**](https://carc.usc.edu/user-information/user-guides/software-and-programming/lmod))
    
3.  It has multiple versions of R installed. Use your favorite one by running

    ``` {.bash}
    module load usc r/[version number]
    ```

    Where `[version number]` can be 3.5.6 and up to 4.0.3 (the latest update). The `usc` module automatically loads gcc/8.3.0, openblas/0.3.8, openmpi/4.0.2, and pmix/3.1.3.

3.  It is never a good idea to use your home directory to install R packages,
    that's why you should try using a
    [**symbolic link instead**](https://en.wikipedia.org/wiki/Symbolic_link),
    like this

    ``` {.bash}
    cd ~
    mkdir -p /path/to/a/project/with/lots/of/space/R
    ln -s /path/to/a/project/with/lots/of/space/R R
    ```

    This way, whenever you install your R packages, R will default to that location

4.  You can run interactive sessions on HPC, but this recommended to be done using
    the `salloc` function in Slurm, in other words, NEVER EVER USE R (OR ANY SOFTWARE)
    TO DO DATA ANALYSIS IN THE HEAD NODES! The options passed to salloc are the same
    options that can be passed to `sbatch` (see the next section.) For example, if
    need to do some analyses in the `thomas` partition (which is private and I have
    access to), I would type something like

    ``` {.bash}
    salloc --account=lc_pdt --partition=thomas --time=02:00:00 --mem-per-cpu=2G
    ```

    This would put me in a single node allocating 2 gigs of memory for a maximum of 2 hours.

